{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Fine-tuning + Distillation + Quantization**\n",
    "In this project, we aim to build two models—a larger “teacher” model (BERT) and a smaller “student” model (DistilBERT)—to classify text as either AI-generated or human-written. Our goal is to make both models accurate while optimizing the student model for efficiency, making it suitable for deployment on devices with limited resources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> <b>Data Preparation</b> </h4>\n",
    "<h5> Data Sources </h5>\n",
    "<ol>\n",
    "  <li> <a href=\"https://www.kaggle.com/datasets/sunilthite/llm-detect-ai-generated-text-dataset\"> LLM - Detect AI Generated Text Dataset </a> </li>\n",
    "  <li> <a href=\"https://www.kaggle.com/datasets/thedrcat/daigt-v2-train-dataset\"> DAIGT V2 Train Dataset </a> </li>\n",
    "</ol>\n",
    "\n",
    "<p>Both of these datasets are combined in a preprocessing step to create a single training dataset.</p>\n",
    "<p>The merging and formatting process is available in a dedicated <a href=\"https://www.kaggle.com/code/openmihirpatel/aivsog-dataprep\"> Kaggle Notebook for Data Preparation </a>.</p>\n",
    "<p>The output of this step is a CSV file containing labeled text data for training and validation.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Fine-tuning and Distillation</b></h3>\n",
    "<h5><b>Fine-tuning</b></h5>\n",
    "<p>We start with <b>BERT-base</b> as our teacher model. After fine-tuning it on our classification task, BERT can accurately differentiate between AI-generated and human-written text. This fine-tuned model serves as a knowledgeable 'teacher.'</p>\n",
    "\n",
    "<h5><b>Distillation</b></h5>\n",
    "<p>We then use <b>DistilBERT-base</b> as our student model. Distillation is the process where the student model learns from the teacher's output, allowing it to achieve similar performance but with reduced complexity and memory usage. We effectively 'compress' the teacher's knowledge into the student model.</p>\n",
    "\n",
    "<p>More details on this process can be found in our notebook on <a href=\"https://www.kaggle.com/code/openmihirpatel/finetuning-and-distillation\"> Model Distillation and Quantization </a>.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Distillation**\n",
    "\n",
    "**Distillation** is a technique to compress a large, complex model (often called the **teacher model**) into a smaller, faster model (known as the **student model**). This process allows the student model to learn from the output predictions of the teacher model, retaining much of the original accuracy while being more efficient.\n",
    "\n",
    "The teacher model is trained on a specific task, such as text classification, and is then used to train the student model. The student model learns to mimic the teacher, making it possible to achieve similar accuracy with reduced model size and computational needs.\n",
    "\n",
    "**Benefits**:\n",
    "- **Improved Efficiency**: Student models are smaller and faster.\n",
    "- **Retained Performance**: Often achieves accuracy close to the teacher model.\n",
    "- **Scalable Deployment**: Useful for real-time applications needing faster response times.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Quantization**\n",
    "\n",
    "**Quantization** is a process that reduces the computational complexity and memory usage of a model by compressing it. In essence, quantization converts the model's parameters from higher precision (e.g., 32-bit floats) to lower precision (e.g., 8-bit integers), which decreases the model size and speeds up inference times. Quantization is especially useful in deploying models to edge devices with limited hardware resources since it can lead to faster and more efficient model performance.\n",
    "\n",
    "**Benefits**:\n",
    "- **Reduces Model Size**: Decreases the storage needed for the model.\n",
    "- **Speeds Up Inference**: Reduces the time taken to make predictions.\n",
    "- **Energy Efficiency**: Useful for low-power devices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5><b>Results Discussion</b></h5>\n",
    "\n",
    "<p>In the table below, we summarize the key results of our models (original and quantized), comparing them based on accuracy, memory size, and prediction speed. By reviewing these metrics, we gain insights into the trade-offs between model size, speed, and performance.</p>\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Model Type</th>\n",
    "    <th>Accuracy</th>\n",
    "    <th>Loss</th>\n",
    "    <th>Params</th>\n",
    "    <th>Size (MB)</th>\n",
    "    <th>Time (ms)</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>BERT-base-uncased</td>\n",
    "    <td>0.993515</td>\n",
    "    <td>0.025127</td>\n",
    "    <td>109.48 M</td>\n",
    "    <td>438.003 MB</td>\n",
    "    <td>495.485 ms</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Distilled-BERT-base-uncased</td>\n",
    "    <td>0.990813</td>\n",
    "    <td>0.040144</td>\n",
    "    <td>66.36 M</td>\n",
    "    <td>265.490 MB</td>\n",
    "    <td>299.864 ms</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Quantized BERT-base-uncased</td>\n",
    "    <td>0.997027</td>\n",
    "    <td>0.019512</td>\n",
    "    <td>109.48 M</td>\n",
    "    <td>181.483 MB</td>\n",
    "    <td>340.033 ms</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Quantized Distilled-BERT-base-uncased</td>\n",
    "    <td>0.977033</td>\n",
    "    <td>0.065918</td>\n",
    "    <td>66.36 M</td>\n",
    "    <td>138.112 MB</td>\n",
    "    <td>191.589 ms</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
